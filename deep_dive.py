import json
import requests
import pickle

# ====== ×˜×¢×™× ×ª × ×ª×•× ×™× ======
ENGINE_URL = "http://127.0.0.1:8080/search"

with open("queries_train.json") as f:
    ground_truth = json.load(f)

with open('id2title.pkl', 'rb') as f:
    id_to_title = pickle.load(f)


def deep_dive_analysis(query):
    """× ×™×ª×•×— ×¢×•××§ ×©×œ ×©××™×œ×ª×” ××—×ª"""

    if query not in ground_truth:
        print(f"âŒ Query '{query}' not found in ground truth!")
        return

    relevant_docs = set(ground_truth[query])

    # ×©×œ×™×¤×ª ×ª×•×¦××•×ª ××”×× ×•×¢
    response = requests.get(ENGINE_URL, params={"query": query})
    search_results = response.json()

    # ×—×™×œ×•×¥ top-10 ×•-top-100
    top_10 = [doc_id for doc_id, _ in search_results[:10]]
    top_100 = [doc_id for doc_id, _ in search_results[:100]]

    # ×—×™×©×•×‘ ××“×“×™×
    hits_10 = len(set(top_10) & relevant_docs)
    hits_100 = len(set(top_100) & relevant_docs)

    print(f"\n{'=' * 100}")
    print(f"ğŸ” DEEP DIVE ANALYSIS: \"{query}\"")
    print(f"{'=' * 100}")

    print(f"\nğŸ“Š Summary:")
    print(f"   Relevant documents in ground truth: {len(relevant_docs)}")
    print(f"   Hits in top-10:  {hits_10}/10 ({hits_10 / 10 * 100:.1f}%)")
    print(f"   Hits in top-100: {hits_100}/100 ({hits_100 / len(relevant_docs) * 100:.1f}% of all relevant)")
    print(f"   Missing from top-100: {len(relevant_docs) - hits_100}")

    print(f"\nğŸ“‹ Top-10 Results with Analysis:")
    print(f"{'Rank':<6} {'Doc ID':<12} {'Relevant?':<12} {'Title'}")
    print("-" * 100)

    for i, doc_id in enumerate(top_10, 1):
        title = id_to_title.get(int(doc_id), "Unknown")
        is_relevant = "âœ… YES" if doc_id in relevant_docs else "âŒ NO"
        print(f"{i:<6} {doc_id:<12} {is_relevant:<12} {title}")

    # × ×™×ª×•×— ××¡××›×™× ×¨×œ×•×•× ×˜×™×™× ×©×œ× × ××¦××• ×‘-top-10
    missing_from_top10 = relevant_docs - set(top_10)

    if missing_from_top10:
        print(f"\nâš ï¸  Relevant documents MISSING from top-10:")
        print(f"{'Doc ID':<12} {'Position':<10} {'Title'}")
        print("-" * 100)

        for doc_id in list(missing_from_top10)[:10]:  # ×”×¦×’ ×¢×“ 10 ×¨××©×•× ×™×
            # ××¦× ××ª ×”××™×§×•× ×©×œ ×”××¡××š (×× ×”×•× ×‘-top-100)
            try:
                pos = top_100.index(doc_id) + 1
                position = f"#{pos}"
            except ValueError:
                position = ">100"

            title = id_to_title.get(int(doc_id), "Unknown")
            print(f"{doc_id:<12} {position:<10} {title}")

    # × ×™×ª×•×— ××¡××›×™× ×œ× ×¨×œ×•×•× ×˜×™×™× ×‘-top-10
    false_positives = set(top_10) - relevant_docs

    if false_positives:
        print(f"\nâŒ Non-relevant documents in top-10 (False Positives):")
        print(f"{'Rank':<6} {'Doc ID':<12} {'Title'}")
        print("-" * 100)

        for i, doc_id in enumerate(top_10, 1):
            if doc_id in false_positives:
                title = id_to_title.get(int(doc_id), "Unknown")
                print(f"{i:<6} {doc_id:<12} {title}")

    print(f"\n{'=' * 100}\n")


# ====== ×“×•×’×××•×ª ×©×™××•×© ======
if __name__ == "__main__":
    # ×‘×—×¨ ×©××™×œ×ª×•×ª ×œ× ×™×ª×•×—

    # ×“×•×’××” 1: ×©××™×œ×ª×” ×©×¢×•×‘×“×ª ×˜×•×‘ (×ª×—×œ×™×£ ×‘×©××™×œ×ª×” ×××™×ª×™×ª ××”× ×™×ª×•×—)
    print("ğŸ¯ ANALYSIS 1: Well-Performing Query")
    deep_dive_analysis("Fossil fuels climate change")

    # ×“×•×’××” 2: ×©××™×œ×ª×” ×©×¢×•×‘×“×ª ×’×¨×•×¢ (×ª×—×œ×™×£ ×‘×©××™×œ×ª×” ×××™×ª×™×ª ××”× ×™×ª×•×—)
    print("ğŸ¯ ANALYSIS 2: Poorly-Performing Query")
    deep_dive_analysis("Printing press invention Gutenberg")