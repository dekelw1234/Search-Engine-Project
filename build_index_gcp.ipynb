{"cells": [{"cell_type": "markdown", "id": "a00e032c", "metadata": {"id": "a00e032c"}, "source": "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"}, {"cell_type": "code", "execution_count": 1, "id": "5ac36d3a", "metadata": {"id": "5ac36d3a", "nbgrader": {"grade": false, "grade_id": "cell-Worker_Count", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\ncluster-0016  GCE       2                                             RUNNING  us-central1-a\n"}], "source": "# if the following command generates an error, you probably didn't enable\n# the cluster security option \"Allow API access to all Google Cloud services\"\n# under Manage Security \u2192 Project Access when setting up the cluster\n!gcloud dataproc clusters list --region us-central1"}, {"cell_type": "markdown", "id": "51cf86c5", "metadata": {"id": "51cf86c5"}, "source": "# Imports & Setup"}, {"cell_type": "code", "execution_count": 2, "id": "bf199e6a", "metadata": {"id": "bf199e6a", "nbgrader": {"grade": false, "grade_id": "cell-Setup", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip install -q google-cloud-storage==1.43.0\n!pip install -q graphframes"}, {"cell_type": "code", "execution_count": 3, "id": "d8f56ecd", "metadata": {"id": "d8f56ecd", "nbgrader": {"grade": false, "grade_id": "cell-Imports", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}, {"data": {"text/plain": "True"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "import pyspark\nimport sys\nfrom collections import Counter, OrderedDict, defaultdict\nimport itertools\nfrom itertools import islice, count, groupby\nimport pandas as pd\nimport os\nimport re\nfrom operator import itemgetter\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.corpus import stopwords\nfrom time import time\nfrom pathlib import Path\nimport pickle\nimport pandas as pd\nfrom google.cloud import storage\n\nimport hashlib\ndef _hash(s):\n    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n\nnltk.download('stopwords')"}, {"cell_type": "code", "execution_count": 4, "id": "38a897f2", "metadata": {"id": "38a897f2", "nbgrader": {"grade": false, "grade_id": "cell-jar", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r-- 1 root root 247882 Jan  6 08:46 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\n"}], "source": "# if nothing prints here you forgot to include the initialization script when starting the cluster\n!ls -l /usr/lib/spark/jars/graph*"}, {"cell_type": "code", "execution_count": 5, "id": "47900073", "metadata": {"id": "47900073", "nbgrader": {"grade": false, "grade_id": "cell-pyspark-import", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "from pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf, SparkFiles\nfrom pyspark.sql import SQLContext\nfrom graphframes import *"}, {"cell_type": "code", "execution_count": 6, "id": "72bed56b", "metadata": {"id": "72bed56b", "nbgrader": {"grade": false, "grade_id": "cell-spark-version", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8", "scrolled": true}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-0016-m.c.ex3-sagikatan.internal:42563\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fbadee27e20>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 7, "id": "980e62a5", "metadata": {"id": "980e62a5", "nbgrader": {"grade": false, "grade_id": "cell-bucket_name", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading data from: ['gs://ex3-sagikatan-bucket/multistream*_preprocessed.parquet']\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 2:=====================================================> (121 + 3) / 124]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of documents to process: 6348910\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# \u05d4\u05d2\u05d3\u05e8\u05ea \u05d4-Bucket \u05e9\u05dc\u05da\nbucket_name = 'ex3-sagikatan-bucket'\n\n# \u05e9\u05d9\u05de\u05d5\u05e9 \u05d1-Wildcard (*) \u05db\u05d3\u05d9 \u05dc\u05e7\u05e8\u05d5\u05d0 \u05e8\u05e7 \u05d0\u05ea \u05e7\u05d1\u05e6\u05d9 \u05d4\u05e0\u05ea\u05d5\u05e0\u05d9\u05dd \u05e9\u05dc \u05d5\u05d9\u05e7\u05d9\u05e4\u05d3\u05d9\u05d4\n# \u05d6\u05d4 \u05d9\u05e1\u05e0\u05df \u05d4\u05d7\u05d5\u05e6\u05d4 \u05d0\u05d5\u05d8\u05d5\u05de\u05d8\u05d9\u05ea \u05d0\u05ea \u05ea\u05d9\u05e7\u05d9\u05d9\u05ea postings_gcp \u05d4\u05d9\u05e9\u05e0\u05d4 \u05d5\u05d0\u05ea \u05d4-PageRank\npaths = [f\"gs://{bucket_name}/multistream*_preprocessed.parquet\"]\n# 2. \u05d9\u05e6\u05d9\u05e8\u05ea \u05d4-Client (\u05d4\u05e9\u05d5\u05e8\u05d5\u05ea \u05d4\u05d7\u05e1\u05e8\u05d5\u05ea)\nfrom google.cloud import storage\nclient = storage.Client()\n# \u05d1\u05d3\u05d9\u05e7\u05d4 \u05de\u05d4\u05d9\u05e8\u05d4\nprint(f\"Reading data from: {paths}\")\nparquetFile = spark.read.parquet(*paths)\nprint(f\"Number of documents to process: {parquetFile.count()}\")"}, {"cell_type": "markdown", "id": "cac891c2", "metadata": {"id": "cac891c2"}, "source": "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."}, {"cell_type": "markdown", "id": "582c3f5e", "metadata": {"id": "582c3f5e"}, "source": "# Building an inverted index"}, {"cell_type": "markdown", "id": "481f2044", "metadata": {"id": "481f2044"}, "source": "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."}, {"cell_type": "code", "execution_count": 8, "id": "e622f989-0f2c-4e62-8317-2483e659a109", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Overwriting inverted_index_gcp.py\n"}], "source": "%%writefile inverted_index_gcp.py\nimport pyspark\nimport sys\nfrom collections import Counter, OrderedDict\nimport itertools\nfrom itertools import islice, count, groupby\nimport pandas as pd\nimport os\nimport re\nfrom operator import itemgetter\nfrom time import time\nfrom pathlib import Path\nimport pickle\nfrom google.cloud import storage\nfrom collections import defaultdict\nfrom contextlib import closing\n\nBLOCK_SIZE = 1999998\n\nclass MultiFileWriter:\n    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n    def __init__(self, base_dir, name, bucket_name):\n        self._base_dir = Path(base_dir)\n        self._name = name\n        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n                          for i in itertools.count())\n        self._f = next(self._file_gen)\n        # Connecting to google storage bucket. \n        self.client = storage.Client()\n        self.bucket = self.client.bucket(bucket_name)\n    \n    def write(self, b):\n        locs = []\n        while len(b) > 0:\n            pos = self._f.tell()\n            remaining = BLOCK_SIZE - pos\n            if remaining == 0:  \n                self._f.close()\n                self.upload_to_gcp()                \n                self._f = next(self._file_gen)\n                pos, remaining = 0, BLOCK_SIZE\n            self._f.write(b[:remaining])\n            locs.append((self._f.name, pos))\n            b = b[remaining:]\n        return locs\n\n    def close(self):\n        self._f.close()\n    \n    def upload_to_gcp(self):\n        '''\n            The function saves the posting files into the right bucket in google storage.\n        '''\n        file_name = self._f.name\n        # UPDATED: Writing to v2 folder\n        blob = self.bucket.blob(f\"postings_gcp_v2/{file_name}\")\n        blob.upload_from_filename(file_name)\n\nclass MultiFileReader:\n    \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n    def __init__(self):\n        self._open_files = {}\n\n    def read(self, locs, n_bytes):\n        b = []\n        for f_name, offset in locs:\n            if f_name not in self._open_files:\n                self._open_files[f_name] = open(f_name, 'rb')\n            f = self._open_files[f_name]\n            f.seek(offset)\n            n_read = min(n_bytes, BLOCK_SIZE - offset)\n            b.append(f.read(n_read))\n            n_bytes -= n_read\n        return b''.join(b)\n  \n    def close(self):\n        for f in self._open_files.values():\n            f.close()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n        return False \n\nTUPLE_SIZE = 6       \nTF_MASK = 2 ** 16 - 1 \n\nclass InvertedIndex:  \n    def __init__(self, docs={}):\n        self.df = Counter()\n        self.term_total = Counter()\n        self._posting_list = defaultdict(list)\n        self.posting_locs = defaultdict(list)\n\n        for doc_id, tokens in docs.items():\n            self.add_doc(doc_id, tokens)\n\n    def add_doc(self, doc_id, tokens):\n        w2cnt = Counter(tokens)\n        self.term_total.update(w2cnt)\n        for w, cnt in w2cnt.items():\n            self.df[w] = self.df.get(w, 0) + 1\n            self._posting_list[w].append((doc_id, cnt))\n\n    def write_index(self, base_dir, name):\n        self._write_globals(base_dir, name)\n\n    def _write_globals(self, base_dir, name):\n        with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n            pickle.dump(self, f)\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        del state['_posting_list']\n        return state\n\n    def posting_lists_iter(self):\n        with closing(MultiFileReader()) as reader:\n            for w, locs in self.posting_locs.items():\n                b = reader.read(locs[0], self.df[w] * TUPLE_SIZE)\n                posting_list = []\n                for i in range(self.df[w]):\n                    doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n                    tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n                    posting_list.append((doc_id, tf))\n                yield w, posting_list\n\n    @staticmethod\n    def read_index(base_dir, name):\n        with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n            return pickle.load(f)\n\n    @staticmethod\n    def delete_index(base_dir, name):\n        path_globals = Path(base_dir) / f'{name}.pkl'\n        path_globals.unlink()\n        for p in Path(base_dir).rglob(f'{name}_*.bin'):\n            p.unlink()\n\n    @staticmethod\n    def write_a_posting_list(b_w_pl, bucket_name):\n        posting_locs = defaultdict(list)\n        bucket_id, list_w_pl = b_w_pl\n        \n        with closing(MultiFileWriter(\".\", bucket_id, bucket_name)) as writer:\n            for w, pl in list_w_pl: \n                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n                              for doc_id, tf in pl])\n                locs = writer.write(b)\n                posting_locs[w].extend(locs)\n            writer.upload_to_gcp() \n            InvertedIndex._upload_posting_locs(bucket_id, posting_locs, bucket_name)\n        return bucket_id\n\n    @staticmethod\n    def _upload_posting_locs(bucket_id, posting_locs, bucket_name):\n        with open(f\"{bucket_id}_posting_locs.pickle\", \"wb\") as f:\n            pickle.dump(posting_locs, f)\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n        # UPDATED: Writing to v2 folder\n        blob_posting_locs = bucket.blob(f\"postings_gcp_v2/{bucket_id}_posting_locs.pickle\")\n        blob_posting_locs.upload_from_filename(f\"{bucket_id}_posting_locs.pickle\")"}, {"cell_type": "code", "execution_count": 9, "id": "e4c523e7", "metadata": {"id": "e4c523e7", "outputId": "3db210ce-3439-4f9e-e71d-b5261c195e65"}, "outputs": [], "source": "parquetFile = spark.read.parquet(*paths)\ndoc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"}, {"cell_type": "markdown", "id": "0d7e2971", "metadata": {"id": "0d7e2971"}, "source": "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"}, {"cell_type": "code", "execution_count": 10, "id": "82881fbf", "metadata": {"id": "82881fbf", "outputId": "ce808f97-dd04-4425-a22f-71c07371faa6"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "6348910"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# Count number of wiki pages\nparquetFile.count()"}, {"cell_type": "markdown", "id": "701811af", "metadata": {"id": "701811af"}, "source": "Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."}, {"cell_type": "code", "execution_count": 11, "id": "121fe102", "metadata": {"id": "121fe102", "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "inverted_index_gcp.py\n"}], "source": "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n%cd -q /home/dataproc\n!ls inverted_index_gcp.py"}, {"cell_type": "code", "execution_count": 12, "id": "57c101a8", "metadata": {"id": "57c101a8", "scrolled": true}, "outputs": [], "source": "# adding our python module to the cluster\nsc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\nsys.path.insert(0,SparkFiles.getRootDirectory())"}, {"cell_type": "code", "execution_count": 13, "id": "c259c402", "metadata": {"id": "c259c402"}, "outputs": [], "source": "from inverted_index_gcp import InvertedIndex"}, {"cell_type": "markdown", "id": "5540c727", "metadata": {"id": "5540c727"}, "source": "**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n\nA few notes:\n1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n3. You are not allowed to change any of the code not coming from Colab."}, {"cell_type": "code", "execution_count": 14, "id": "f3ad8fea", "metadata": {"id": "f3ad8fea", "nbgrader": {"grade": false, "grade_id": "cell-token2bucket", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "english_stopwords = frozenset(stopwords.words('english'))\ncorpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n                    \"many\", \"however\", \"would\", \"became\"]\n\nall_stopwords = english_stopwords.union(corpus_stopwords)\nRE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n\nNUM_BUCKETS = 124\ndef token2bucket_id(token):\n  return int(_hash(token),16) % NUM_BUCKETS\n\n# PLACE YOUR CODE HERE\ndef word_count(text, id):\n  ''' Count the frequency of each word in `text` (tf) that is not included in\n  `all_stopwords` and return entries that will go into our posting lists.\n  Parameters:\n  -----------\n    text: str\n      Text of one document\n    id: int\n      Document id\n  Returns:\n  --------\n    List of tuples\n      A list of (token, (doc_id, tf)) pairs\n      for example: [(\"Anarchism\", (12, 5)), ...]\n  '''\n  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n  # YOUR CODE HERE\n  filtered_tokens = [t for t in tokens if t not in all_stopwords] # remove stop-words\n  freqs = Counter(filtered_tokens) # calculate tf with counter\n  return [(token, (id, tf)) for token, tf in freqs.items()]\n\n\ndef reduce_word_counts(unsorted_pl):\n  ''' Returns a sorted posting list by wiki_id.\n  Parameters:\n  -----------\n    unsorted_pl: list of tuples\n      A list of (wiki_id, tf) tuples\n  Returns:\n  --------\n    list of tuples\n      A sorted posting list.\n  '''\n  # unsorted_pl is a list of (wiki_id, tf)\n  return sorted(unsorted_pl, key=lambda x: x[0])\n\ndef calculate_df(postings):\n  ''' Takes a posting list RDD and calculate the df for each token.\n  Parameters:\n  -----------\n    postings: RDD\n      An RDD where each element is a (token, posting_list) pair.\n  Returns:\n  --------\n    RDD\n      An RDD where each element is a (token, df) pair.\n  '''\n  # YOUR CODE HERE\n  return postings.mapValues(lambda pl: len(pl))\n\n\n# --------------------------------------------------------------------------------\n# Updated partition function to support multiple indices (title, anchor, body)\n# --------------------------------------------------------------------------------\n\ndef partition_postings_and_write(postings, index_name):\n  ''' A function that partitions the posting lists into buckets, writes out\n  all posting lists in a bucket to disk, and returns the posting locations for\n  each bucket.\n\n  Parameters:\n  -----------\n    postings: RDD\n      An RDD where each item is a (w, posting_list) pair.\n    index_name: str\n      A string identifier (e.g., \"body\", \"title\", \"anchor\") to prefix the bucket IDs.\n  Returns:\n  --------\n    RDD\n      An RDD where each item is a posting locations dictionary for a bucket.\n  '''\n  # Map to (bucket_id, (w, pl)), where bucket_id is a STRING like \"body_5\"\n  bucketed = postings.map(lambda x: (f\"{index_name}_{token2bucket_id(x[0])}\", x))\n\n  # Group by the string bucket_id\n  grouped = bucketed.groupByKey().mapValues(list)\n\n  # Write using the existing static method\n  return grouped.map(\n        lambda pair: InvertedIndex.write_a_posting_list(\n            pair,\n            bucket_name=bucket_name\n        )\n    )\n# new functions\n\ndef extract_anchor_text_to_target(row):\n    '''\n    Input: Row(id=source_doc_id, anchor_text=[Row(id=target_id, text=\"link text\"), ...])\n    Output: List of (target_id, link_text_str)\n    '''\n    results = []\n    # Iterate over the links in the current page\n    for link in row.anchor_text:\n        # We care about the target_id (link.id) and the text used to link to it\n        if link.id is not None:\n            results.append((link.id, link.text))\n    return results\n\ndef run_index_creation(rdd_text_id, index_name, min_tf_cutoff=10):\n    t_start = time()\n    print(f\"--- Starting {index_name} index construction ---\")\n    \n    word_counts = rdd_text_id.flatMap(lambda x: word_count(x[0], x[1]))\n    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n    postings_filtered = postings.filter(lambda x: len(x[1]) > min_tf_cutoff)\n    w2df = calculate_df(postings_filtered)\n    w2df_dict = w2df.collectAsMap()\n    \n    # Writing posting lists (this uses the updated .py file logic internally)\n    _ = partition_postings_and_write(postings_filtered, index_name).collect()\n    \n    # Collecting posting locs - scan the NEW folder\n    # \u05e9\u05d9\u05dd \u05dc\u05d1: \u05e2\u05d3\u05db\u05e0\u05ea\u05d9 \u05db\u05d0\u05df \u05dc\u05ea\u05d9\u05e7\u05d9\u05d9\u05d4 \u05d4\u05d7\u05d3\u05e9\u05d4\n    target_folder = 'postings_gcp_v2' \n    \n    super_posting_locs = defaultdict(list)\n    for blob in client.list_blobs(bucket_name, prefix=target_folder):\n        if not blob.name.endswith(\"pickle\"):\n            continue\n        filename = os.path.basename(blob.name)\n        if not filename.startswith(index_name + \"_\"):\n            continue     \n        with blob.open(\"rb\") as f:\n            posting_locs = pickle.load(f)\n            for k, v in posting_locs.items():\n                super_posting_locs[k].extend(v)\n                \n    inverted = InvertedIndex()\n    inverted.posting_locs = super_posting_locs\n    inverted.df = w2df_dict\n    inverted.write_index('.', f'index_{index_name}')\n    \n    # Upload global index to the NEW folder\n    index_src = f\"index_{index_name}.pkl\"\n    index_dst = f'gs://{bucket_name}/{target_folder}/{index_src}'\n    os.system(f\"gsutil cp {index_src} {index_dst}\")\n    \n    print(f\"--- Finished {index_name} index. Time: {time() - t_start} seconds ---\")\n    return inverted\n\n"}, {"cell_type": "code", "execution_count": null, "id": "AFJA2CKTyy-a", "metadata": {"id": "AFJA2CKTyy-a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--- Processing TITLE ---\n--- Starting title index construction ---\n"}, {"name": "stderr", "output_type": "stream", "text": "Copying file://index_title.pkl [Content-Type=application/octet-stream]...       \n- [1 files][ 67.6 MiB/ 67.6 MiB]                                                \nOperation completed over 1 objects/67.6 MiB.                                     \n"}, {"name": "stdout", "output_type": "stream", "text": "--- Finished title index. Time: 87.98648929595947 seconds ---\n--- Processing ANCHOR ---\n--- Starting anchor index construction ---\n"}, {"name": "stderr", "output_type": "stream", "text": "Copying file://index_anchor.pkl [Content-Type=application/octet-stream]...      \n/ [1 files][  7.9 MiB/  7.9 MiB]                                                \nOperation completed over 1 objects/7.9 MiB.                                      \n"}, {"name": "stdout", "output_type": "stream", "text": "--- Finished anchor index. Time: 1368.455739736557 seconds ---\n--- Processing BODY ---\n--- Starting body index construction ---\n"}, {"name": "stderr", "output_type": "stream", "text": "Copying file://index_body.pkl [Content-Type=application/octet-stream]...        \n/ [1 files][ 18.5 MiB/ 18.5 MiB]                                                \nOperation completed over 1 objects/18.5 MiB.                                     \n"}, {"name": "stdout", "output_type": "stream", "text": "--- Finished body index. Time: 3377.511700630188 seconds ---\n"}], "source": "# Load the parquet files once\nparquetFile = spark.read.parquet(*paths)\n\n# ==========================================\n# 1. Build TITLE Index (The lightest task)\n# ==========================================\nprint(\"--- Processing TITLE ---\")\n# Select title and id\ntitle_rdd = parquetFile.select(\"title\", \"id\").rdd\n# Run pipeline\ntitle_index = run_index_creation(title_rdd, \"title\", min_tf_cutoff=0)\n\n\n# ==========================================\n# 2. Build ANCHOR Index (Medium task, involves shuffle)\n# ==========================================\nprint(\"--- Processing ANCHOR ---\")\n# This requires grouping all anchor texts pointing TO a specific doc_id\nanchor_data = parquetFile.select(\"id\", \"anchor_text\").rdd\n\n# Extract (target_id, link_text) pairs\nanchor_rdd_grouped = anchor_data.flatMap(extract_anchor_text_to_target) \\\n                                .groupByKey() \\\n                                .mapValues(lambda x: \" \".join(x))\n\n# Swap to (text, id) format\nanchor_rdd_ready = anchor_rdd_grouped.map(lambda x: (x[1], x[0]))\n\n# --- \u05ea\u05d9\u05e7\u05d5\u05df: \u05de\u05e8\u05d9\u05e6\u05d9\u05dd \u05d0\u05ea \u05d4\u05d9\u05e6\u05d9\u05e8\u05d4 \u05db\u05d0\u05df, \u05dc\u05e4\u05e0\u05d9 \u05e9\u05e2\u05d5\u05d1\u05e8\u05d9\u05dd \u05dc-BODY ---\nanchor_index = run_index_creation(anchor_rdd_ready, \"anchor\", min_tf_cutoff=5)\n\n\n# ==========================================\n# 3. Build BODY Index (The heaviest task)\n# ==========================================\nprint(\"--- Processing BODY ---\")\n# Select text and id\nbody_rdd = parquetFile.select(\"text\", \"id\").rdd\n# Run pipeline\nbody_index = run_index_creation(body_rdd, \"body\", min_tf_cutoff=50)"}, {"cell_type": "markdown", "id": "f6f66e3a", "metadata": {"id": "f6f66e3a"}, "source": "Putting it all together"}, {"cell_type": "markdown", "id": "c52dee14", "metadata": {"id": "c52dee14", "nbgrader": {"grade": false, "grade_id": "cell-2a6d655c112e79c5", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# PageRank"}, {"cell_type": "markdown", "id": "0875c6bd", "metadata": {"id": "0875c6bd", "nbgrader": {"grade": false, "grade_id": "cell-2fee4bc8d83c1e2a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**YOUR TASK (10 POINTS):** Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below."}, {"cell_type": "code", "execution_count": null, "id": "31a516e2", "metadata": {"id": "31a516e2"}, "outputs": [], "source": "# Put your `generate_graph` function here\ndef generate_graph(pages):\n    edges = (\n        pages.flatMap(\n            lambda row: [\n                (row.id, link.id)\n                for link in row.anchor_text\n                if link.id is not None\n            ]\n        ).distinct()\n    )\n\n    vertices = (\n        edges.flatMap(lambda e: [(e[0],), (e[1],)])\n              .distinct()\n    )\n\n    return edges, vertices\n"}, {"cell_type": "code", "execution_count": null, "id": "a8f13047-8c75-4120-8d80-a19a29415c89", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "6bc05ba3", "metadata": {"id": "6bc05ba3", "nbgrader": {"grade": false, "grade_id": "cell-PageRank", "locked": false, "schema_version": 3, "solution": true, "task": false}, "outputId": "54497d88-b6af-474d-ccfa-2308c3a6f645"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"ename": "AnalysisException", "evalue": "path gs://ex3-sagikatan-bucket/pr already exists.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr_results\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr\u001b[38;5;241m.\u001b[39msort(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/pr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pr_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m     14\u001b[0m pr\u001b[38;5;241m.\u001b[39mshow()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1372\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1366\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m   1367\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m   1371\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mAnalysisException\u001b[0m: path gs://ex3-sagikatan-bucket/pr already exists."]}], "source": "t_start = time()\npages_links = spark.read.parquet(\"gs://ex3-sagikatan-bucket/multistream*\").select(\"id\", \"anchor_text\").rdd\n# construct the graph\nedges, vertices = generate_graph(pages_links)\n# compute PageRank\nedgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\nverticesDF = vertices.toDF(['id']).repartition(124, 'id')\ng = GraphFrame(verticesDF, edgesDF)\npr_results = g.pageRank(resetProbability=0.15, maxIter=6)\npr = pr_results.vertices.select(\"id\", \"pagerank\")\npr = pr.sort(col('pagerank').desc())\npr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\npr_time = time() - t_start\npr.show()"}, {"cell_type": "code", "execution_count": 18, "id": "89fa1f9b-b3bd-455c-960d-d320bc582a45", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "x\n"}], "source": "print(\"x\")"}, {"cell_type": "code", "execution_count": 19, "id": "7ab5b9f4-73ba-47a8-a741-3fff571702a6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--- Current Calculation (Memory) ---\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Rows in memory: 6345849\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+\n|     id|         pagerank|\n+-------+-----------------+\n|3434750|9913.728782160779|\n|  10568|5385.349263642041|\n|  32927|5282.081575765275|\n|  30680|5128.233709604119|\n|5843419|4957.567686263868|\n+-------+-----------------+\nonly showing top 5 rows\n\n\n--- Saved File (Bucket) ---\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 312:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Rows on disk: 6345849\n+-------+------------------+\n|    _c0|               _c1|\n+-------+------------------+\n|3434750| 9913.728782160777|\n|  10568|  5385.34926364204|\n|  32927|5282.0815757652745|\n|  30680| 5128.233709604119|\n|5843419| 4957.567686263868|\n+-------+------------------+\nonly showing top 5 rows\n\n\n\u2705 MATCH: The file on disk seems to have the same number of rows as the calculation.\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# 1. \u05d1\u05d3\u05d9\u05e7\u05ea \u05de\u05d4 \u05e9\u05d9\u05e9 \u05db\u05e8\u05d2\u05e2 \u05d1\u05d6\u05d9\u05db\u05e8\u05d5\u05df (\u05de\u05d4\u05d7\u05d9\u05e9\u05d5\u05d1 \u05d4\u05d0\u05d7\u05e8\u05d5\u05df)\nprint(\"--- Current Calculation (Memory) ---\")\ncount_memory = pr.count()\nprint(f\"Rows in memory: {count_memory}\")\npr.show(5)\n\n# 2. \u05d1\u05d3\u05d9\u05e7\u05ea \u05de\u05d4 \u05e9\u05e9\u05de\u05d5\u05e8 \u05d1-Bucket (\u05de\u05d4\u05e2\u05d1\u05e8)\nprint(\"\\n--- Saved File (Bucket) ---\")\ntry:\n    pr_disk = spark.read.csv(f\"gs://{bucket_name}/pr\", header=False, inferSchema=True)\n    count_disk = pr_disk.count()\n    print(f\"Rows on disk: {count_disk}\")\n    pr_disk.show(5)\n    \n    # \u05de\u05e1\u05e7\u05e0\u05d4\n    if count_memory == count_disk:\n        print(\"\\n\u2705 MATCH: The file on disk seems to have the same number of rows as the calculation.\")\n    else:\n        print(\"\\n\u26a0\ufe0f MISMATCH: The file on disk is different. You should overwrite it.\")\n        \nexcept Exception as e:\n    print(f\"\\n\u274c Error reading from disk (File might be corrupted or empty): {e}\")"}, {"cell_type": "code", "execution_count": null, "id": "f7717604", "metadata": {"id": "f7717604", "nbgrader": {"grade": true, "grade_id": "cell-PageRank_time", "locked": true, "points": 10, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "# test that PageRank computaion took less than 1 hour\nassert pr_time < 60*120"}, {"cell_type": "code", "execution_count": 20, "id": "d202742c-a4d7-417b-a35d-450c07dbba09", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--- Converting PageRank CSV to Pickle ---\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Loaded PageRank for 6345849 documents.\n\u2705 Success! 'pagerank.pkl' is now saved in 'postings_gcp_v2/'\n"}], "source": "import pickle\n\nbucket_name = 'ex3-sagikatan-bucket' \n\nprint(\"--- Converting PageRank CSV to Pickle ---\")\n\ntry:\n    # 1. \u05e7\u05e8\u05d9\u05d0\u05ea \u05e7\u05d5\u05d1\u05e5 \u05d4-CSV \u05d4\u05de\u05db\u05d5\u05d5\u05e5 \u05de\u05d4\u05d1\u05d0\u05e7\u05d8\n    # Spark \u05d9\u05d5\u05d3\u05e2 \u05dc\u05e4\u05ea\u05d5\u05d7 \u05d0\u05ea \u05d4-gz \u05dc\u05d1\u05d3\n    pr_df = spark.read.csv(f\"gs://{bucket_name}/pr\", header=False, inferSchema=True)\n    \n    # 2. \u05d4\u05de\u05e8\u05d4 \u05dc\u05de\u05d9\u05dc\u05d5\u05df (Map) \u05e9\u05dc {doc_id: score}\n    # \u05d6\u05d4 \u05d4\u05d7\u05dc\u05e7 \u05d4\u05d7\u05e9\u05d5\u05d1 - \u05d0\u05d5\u05e1\u05e3 \u05d0\u05ea \u05db\u05dc \u05d4\u05e0\u05ea\u05d5\u05e0\u05d9\u05dd \u05dc\u05d6\u05d9\u05db\u05e8\u05d5\u05df\n    pagerank_dict = pr_df.rdd.map(lambda x: (x[0], x[1])).collectAsMap()\n    \n    print(f\"Loaded PageRank for {len(pagerank_dict)} documents.\")\n\n    # 3. \u05e9\u05de\u05d9\u05e8\u05d4 \u05dc\u05e7\u05d5\u05d1\u05e5 Pickle \u05de\u05e7\u05d5\u05de\u05d9\n    with open(\"pagerank.pkl\", \"wb\") as f:\n        pickle.dump(pagerank_dict, f)\n\n    # 4. \u05d4\u05e2\u05dc\u05d0\u05d4 \u05dc\u05ea\u05d9\u05e7\u05d9\u05d9\u05d4 \u05d4\u05de\u05e1\u05d5\u05d3\u05e8\u05ea (postings_gcp_v2)\n    blob = client.bucket(bucket_name).blob(\"postings_gcp_v2/pagerank.pkl\")\n    blob.upload_from_filename(\"pagerank.pkl\")\n    \n    print(\"\u2705 Success! 'pagerank.pkl' is now saved in 'postings_gcp_v2/'\")\n\nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")"}, {"cell_type": "code", "execution_count": 21, "id": "0f72386b-c349-4016-bc59-f208f654d814", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--- Creating id2title dictionary ---\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Dictionary created successfully with 6348910 entries.\n\u2705 Success! 'id2title.pkl' is now saved in 'postings_gcp_v2/'\n"}], "source": "import pickle\n\n# \u05d5\u05d3\u05d0 \u05e9\u05d4\u05e9\u05dd \u05d4\u05d6\u05d4 \u05e0\u05db\u05d5\u05df\nbucket_name = 'ex3-sagikatan-bucket'\n\nprint(\"--- Creating id2title dictionary ---\")\n\ntry:\n    # \u05e9\u05dc\u05d1 1: \u05d9\u05e6\u05d9\u05e8\u05ea \u05d4\u05de\u05d9\u05dc\u05d5\u05df \u05de\u05ea\u05d5\u05da \u05d4\u05de\u05d9\u05d3\u05e2 \u05e9\u05db\u05d1\u05e8 \u05d8\u05e2\u05d5\u05df \u05d1-Spark\n    # (\u05d0\u05e0\u05d7\u05e0\u05d5 \u05dc\u05d5\u05e7\u05d7\u05d9\u05dd \u05e8\u05e7 \u05d0\u05ea \u05d4\u05e2\u05de\u05d5\u05d3\u05d5\u05ea id \u05d5-title)\n    # \u05d4\u05e2\u05e8\u05d4: \u05d4\u05e4\u05e2\u05d5\u05dc\u05d4 \u05d4\u05d6\u05d5 \u05d0\u05d5\u05e1\u05e4\u05ea \u05db-6 \u05de\u05d9\u05dc\u05d9\u05d5\u05df \u05d6\u05d5\u05d2\u05d5\u05ea \u05dc\u05d6\u05d9\u05db\u05e8\u05d5\u05df, \u05d6\u05d4 \u05e2\u05e9\u05d5\u05d9 \u05dc\u05e7\u05d7\u05ea \u05d3\u05e7\u05d4-\u05e9\u05ea\u05d9\u05d9\u05dd\n    id2title_dict = parquetFile.select(\"id\", \"title\").rdd.collectAsMap()\n    \n    print(f\"Dictionary created successfully with {len(id2title_dict)} entries.\")\n\n    # \u05e9\u05dc\u05d1 2: \u05e9\u05de\u05d9\u05e8\u05d4 \u05dc\u05e7\u05d5\u05d1\u05e5 Pickle \u05de\u05e7\u05d5\u05de\u05d9\n    with open(\"id2title.pkl\", \"wb\") as f:\n        pickle.dump(id2title_dict, f)\n\n    # \u05e9\u05dc\u05d1 3: \u05d4\u05e2\u05dc\u05d0\u05d4 \u05dc-Bucket \u05dc\u05ea\u05d9\u05e7\u05d9\u05d9\u05d4 \u05d4\u05de\u05e1\u05d5\u05d3\u05e8\u05ea\n    blob = client.bucket(bucket_name).blob(\"postings_gcp_v2/id2title.pkl\")\n    blob.upload_from_filename(\"id2title.pkl\")\n\n    print(\"\u2705 Success! 'id2title.pkl' is now saved in 'postings_gcp_v2/'\")\n\nexcept NameError:\n    print(\"\u274c Error: 'parquetFile' is not defined. Please re-run the cell that loads the data (spark.read.parquet).\")\nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")"}, {"cell_type": "code", "execution_count": 24, "id": "3345b06d-6e46-41c2-a209-36f85561f6aa", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--- \ud83c\udfc1 Final Inventory Check for: postings_gcp_v2 ---\nBody Index (Pickle)       \u2705 Found\nBody Locs (Pickle)        \u274c MISSING\nTitle Index (Pickle)      \u2705 Found\nTitle Locs (Pickle)       \u274c MISSING\nAnchor Index (Pickle)     \u2705 Found\nAnchor Locs (Pickle)      \u274c MISSING\nPageRank (Pickle)         \u2705 Found\nID to Title (Pickle)      \u2705 Found\n----------------------------------------\nBinary Files (.bin):      3876 files found (Should be > 0)\n\n\ud83c\udf89 MISSION ACCOMPLISHED! Your bucket is fully ready for the Search Engine.\nYou can safely delete this Dataproc cluster now.\n"}], "source": "from google.cloud import storage\n\n# \u05d5\u05d3\u05d0 \u05e9\u05d6\u05d4 \u05d4\u05e9\u05dd \u05d4\u05e0\u05db\u05d5\u05df\nbucket_name = 'ex3-sagikatan-bucket' \nprefix = 'postings_gcp_v2'\n\nclient = storage.Client()\nblobs = client.list_blobs(bucket_name, prefix=prefix)\nfiles = [b.name for b in blobs]\n\nprint(f\"--- \ud83c\udfc1 Final Inventory Check for: {prefix} ---\")\n\n# \u05e8\u05e9\u05d9\u05de\u05ea \u05d4\u05e7\u05d1\u05e6\u05d9\u05dd \u05e9\u05d7\u05d9\u05d9\u05d1\u05d9\u05dd \u05dc\u05d4\u05d9\u05d5\u05ea \u05e9\u05dd \u05db\u05d3\u05d9 \u05e9\u05d4\u05de\u05e0\u05d5\u05e2 \u05d9\u05e2\u05d1\u05d5\u05d3\nchecklist = {\n    'Body Index (Pickle)': 'index_body.pkl',\n    'Body Locs (Pickle)': 'body_posting_locs.pickle',\n    'Title Index (Pickle)': 'index_title.pkl',\n    'Title Locs (Pickle)': 'title_posting_locs.pickle',\n    'Anchor Index (Pickle)': 'index_anchor.pkl',\n    'Anchor Locs (Pickle)': 'anchor_posting_locs.pickle',\n    'PageRank (Pickle)': 'pagerank.pkl',\n    'ID to Title (Pickle)': 'id2title.pkl'\n}\n\n# --- \u05d4\u05ea\u05d9\u05e7\u05d5\u05df \u05db\u05d0\u05df: \u05e9\u05d9\u05de\u05d5\u05e9 \u05d1-len \u05d1\u05de\u05e7\u05d5\u05dd \u05d1-sum ---\nbin_files_count = len([f for f in files if f.endswith('.bin')])\n\nall_exist = True\n\nfor label, filename in checklist.items():\n    # \u05d1\u05d5\u05d3\u05e7 \u05d0\u05dd \u05e9\u05dd \u05d4\u05e7\u05d5\u05d1\u05e5 \u05e7\u05d9\u05d9\u05dd \u05d1\u05e0\u05ea\u05d9\u05d1 \u05d4\u05de\u05dc\u05d0\n    exists = any(filename in f for f in files)\n    status = \"\u2705 Found\" if exists else \"\u274c MISSING\"\n    print(f\"{label:<25} {status}\")\n    if not exists:\n        all_good = False\n\nprint(\"-\" * 40)\nprint(f\"Binary Files (.bin):      {bin_files_count} files found (Should be > 0)\")\n\nif all_exist and bin_files_count > 0:\n    print(\"\\n\ud83c\udf89 MISSION ACCOMPLISHED! Your bucket is fully ready for the Search Engine.\")\n    print(\"You can safely delete this Dataproc cluster now.\")\nelse:\n    print(\"\\n\u26a0\ufe0f WARNING: Something is missing. Do not delete the cluster yet!\")"}, {"cell_type": "code", "execution_count": 25, "id": "56b814a2-4604-4705-907f-f070ac71b13f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Downloading Body Index for verification...\nCopying gs://ex3-sagikatan-bucket/postings_gcp_v2/index_body.pkl...\n/ [1 files][ 18.5 MiB/ 18.5 MiB]                                                \nOperation completed over 1 objects/18.5 MiB.                                     \n\n--- VERIFICATION RESULTS ---\nBody Index contains locations for: 495515 terms\n\u2705 SUCCESS: The posting locations are safely stored inside the Pickle.\n\ud83d\ude80 You are safe to DELETE the cluster.\n"}], "source": "# \u05d4\u05d5\u05e8\u05d3\u05d4 \u05e9\u05dc \u05d4\u05d0\u05d9\u05e0\u05d3\u05e7\u05e1 \u05d4\u05e8\u05d0\u05e9\u05d9 (Body) \u05dc\u05d1\u05d3\u05d9\u05e7\u05d4\nprint(\"Downloading Body Index for verification...\")\n!gsutil cp gs://{bucket_name}/postings_gcp_v2/index_body.pkl .\n\ntry:\n    # \u05d8\u05e2\u05d9\u05e0\u05ea \u05d4\u05d0\u05d9\u05e0\u05d3\u05e7\u05e1\n    idx_body = InvertedIndex.read_index('.', 'index_body')\n    \n    # \u05d1\u05d3\u05d9\u05e7\u05d4 \u05db\u05de\u05d4 \u05de\u05d9\u05dc\u05d9\u05dd \u05d9\u05e9 \u05dc\u05d4\u05df \u05de\u05d9\u05e7\u05d5\u05de\u05d9\u05dd\n    locs_count = len(idx_body.posting_locs)\n    \n    print(f\"\\n--- VERIFICATION RESULTS ---\")\n    print(f\"Body Index contains locations for: {locs_count} terms\")\n    \n    if locs_count > 1000:\n        print(\"\u2705 SUCCESS: The posting locations are safely stored inside the Pickle.\")\n        print(\"\ud83d\ude80 You are safe to DELETE the cluster.\")\n    else:\n        print(\"\u274c CRITICAL: The index is empty or corrupted. DO NOT DELETE.\")\n        \nexcept Exception as e:\n    print(f\"\u274c Error reading index: {e}\")"}, {"cell_type": "code", "execution_count": null, "id": "33f11255-511a-4419-b019-6ac6f462571d", "metadata": {}, "outputs": [], "source": "# \u05d1\u05d3\u05d9\u05e7\u05d4 \u05e1\u05d5\u05e4\u05d9\u05ea - \u05d4\u05d0\u05dd \u05d4\u05db\u05dc \u05de\u05d5\u05db\u05df \u05dc\u05de\u05e0\u05d5\u05e2 \u05d4\u05d7\u05d9\u05e4\u05d5\u05e9?\nprefix = 'postings_gcp_v2'\nblobs = client.list_blobs(bucket_name, prefix=prefix)\nfiles = [b.name for b in blobs]\n\nprint(f\"--- Final Check for {prefix} ---\")\n\nrequired_files = [\n    'pagerank.pkl',\n    'pageviews-202108-user.pkl',\n    'id2title.pkl',\n    'index_body.pkl',\n    'index_title.pkl',\n    'index_anchor.pkl'\n]\n\nall_good = True\nfor f in required_files:\n    # \u05d1\u05d5\u05d3\u05e7 \u05d0\u05dd \u05d4\u05e7\u05d5\u05d1\u05e5 \u05e7\u05d9\u05d9\u05dd \u05d1\u05ea\u05d5\u05da \u05e8\u05e9\u05d9\u05de\u05ea \u05d4\u05e7\u05d1\u05e6\u05d9\u05dd \u05d1\u05d1\u05d0\u05e7\u05d8\n    exists = any(f in file_path for file_path in files)\n    status = \"\u2705 Found\" if exists else \"\u274c MISSING\"\n    print(f\"{f:<30} {status}\")\n    if not exists:\n        all_good = False\n\nif all_good:\n    print(\"\\n\ud83c\udf89 CONGRATS! You have all files needed for the Search Engine.\")\nelse:\n    print(\"\\n\u26a0\ufe0f Some files are missing. Check the errors above.\")"}, {"cell_type": "markdown", "id": "96e9a610", "metadata": {"id": "96e9a610"}, "source": "# Reporting"}, {"cell_type": "markdown", "id": "a1da57c7", "metadata": {"id": "a1da57c7"}, "source": "**YOUR TASK (5 points):** execute and complete the following lines to complete\nthe reporting requirements for assignment #3."}, {"cell_type": "code", "execution_count": null, "id": "0f0d5523", "metadata": {"id": "0f0d5523", "nbgrader": {"grade": false, "grade_id": "cell-size_ofi_input_data", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "54595c29-4ae3-4b78-86d0-d8457ae9c150"}, "outputs": [], "source": "# size of input data\n!gsutil du -sh \"gs://wikidata_preprocessed/\""}, {"cell_type": "code", "execution_count": null, "id": "ce25a98a", "metadata": {"id": "ce25a98a", "nbgrader": {"grade": false, "grade_id": "cell-size_of_index_data", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "44d9721a-1cd7-4e59-9f78-5439864cfdad"}, "outputs": [], "source": "# size of index data\nindex_dst = f'gs://{bucket_name}/postings_gcp/'\n!gsutil du -sh \"$index_dst\""}, {"cell_type": "code", "execution_count": null, "id": "7a9538ee", "metadata": {"id": "7a9538ee", "nbgrader": {"grade": false, "grade_id": "cell-credits", "locked": true, "schema_version": 3, "solution": false, "task": false}, "outputId": "1be5b09c-aa79-41cc-af00-6eeb3e0432fe"}, "outputs": [], "source": "# How many USD credits did you use in GCP during the course of this assignment?\ncost = 5.92\nprint(f'I used {cost} USD credit during the course of this assignment')"}, {"cell_type": "markdown", "id": "fb0e0ed8", "metadata": {"id": "fb0e0ed8"}, "source": "**Bonus (10 points)** if you implement PageRank in pure PySpark, i.e. without using the GraphFrames package, AND manage to complete 10 iterations of your algorithm on the entire English Wikipedia in less than an hour.\n"}, {"cell_type": "code", "execution_count": null, "id": "b8157868", "metadata": {"id": "b8157868", "nbgrader": {"grade": false, "grade_id": "cell-PageRank_Bonus", "locked": false, "schema_version": 3, "solution": true, "task": false}, "outputId": "f7f50f6f-f510-4906-ee36-163ab1da01b2"}, "outputs": [], "source": "#If you have decided to do the bonus task - please copy the code here\n\nbonus_flag = False # Turn flag on (True) if you have implemented this part\n\nt_start = time()\n\n# PLACE YOUR CODE HERE\n\npr_time_Bonus = time() - t_start\n"}, {"cell_type": "code", "execution_count": null, "id": "855f9c94", "metadata": {"id": "855f9c94", "nbgrader": {"grade": true, "grade_id": "cell-PageRank_Bonus-time", "locked": true, "points": 10, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "# Note:test that PageRank computaion took less than 1 hour\nassert pr_time_Bonus < 60*60 and bonus_flag"}, {"cell_type": "code", "execution_count": null, "id": "40e5bfed", "metadata": {"id": "40e5bfed"}, "outputs": [], "source": ""}], "metadata": {"celltoolbar": "Create Assignment", "colab": {"provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}